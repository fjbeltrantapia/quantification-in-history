[
  {
    "objectID": "speeches.html",
    "href": "speeches.html",
    "title": "Case-Study 2: The State of the Union Presidential Speeches",
    "section": "",
    "text": "This second case-study explores how computational methods help extracting information from unstructured texts. We are only going to introduce very basic tools that basically count words. There are however more sophisticated tools, so take this just a first taste into this topic."
  },
  {
    "objectID": "speeches.html#preliminary-steps",
    "href": "speeches.html#preliminary-steps",
    "title": "Case-Study 2: The State of the Union Presidential Speeches",
    "section": "Preliminary steps",
    "text": "Preliminary steps\nAs explained in the previous section, we need to include some preliminary commands in our script so we (1) get rid of other objects that could be in the R environment from previous sessions, (2) set the working directory, (3) load (and install if necessary) the packages we plan to use, and (4) import the dataset. Apart from the tidyverse, we will also make use of the tidytext package that contains many of the functions to treat textual corpuses. Importin the .csv file containing the corpus into R involves using the command read_csv() which is part of the tidyverse.\n\n# Clear de \"Global Environment\"\nrm(list=ls()) \n\n# Sets the working directory\nsetwd(\"~/Documents/quants\") \n\n# Install/load packages\ninstall.packages(\"tidytext\")\nlibrary(tidyverse)\nlibrary(tidytext)\n\n# Importing the data\nspeeches &lt;- read_csv(\"data/state-of-the-union-texts.csv\")"
  },
  {
    "objectID": "speeches.html#inspecting-the-data",
    "href": "speeches.html#inspecting-the-data",
    "title": "Case-Study 2: The State of the Union Presidential Speeches",
    "section": "Inspecting the data",
    "text": "Inspecting the data\nLet’s start by having a first look at the data itself typing the name of the object we just created (speeches). As shown below, the contents of the tidyverse have been nicely structured into a data frame containing 4 columns (different pieces of information) and 235 rows (one for each speech).\n\nspeeches\n\n# A tibble: 219 × 4\n   President          Year Title                              Text              \n   &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;                              &lt;chr&gt;             \n 1 George Washington  1790 First State of the Union Address   \"['I embrace with…\n 2 George Washington  1790 Second State of the Union Address  \"['Fellow-Citizen…\n 3 George Washington  1791 Third State of the Union Address   \"['Fellow-Citizen…\n 4 George Washington  1792 Fourth State of the Union Address  \"['Fellow-Citizen…\n 5 George Washington  1793 Fifth State of the Union Address   \"['Fellow Citizen…\n 6 George Washington  1794 Sixth State of the Union Address   \"['Fellow Citizen…\n 7 George Washington  1795 Seventh State of the Union Address \"['Fellow Citizen…\n 8 George Washington  1796 Eighth State of the Union Address  \"['Fellow Citizen…\n 9 by John Adams      1797 First State of the Union Address   \"['I was for some…\n10 by John Adams      1798 Second State of the Union Address  \"['Gentlemen of t…\n# ℹ 209 more rows\n\n\nWe can use the functions we are already familiar with to continue exploring the data. We can see for instance that two speeches were delivered in 1790 (there is only one speech each year from then on), something we will need to take into account later.\n\nspeeches |&gt;\n  count(Year)\n\n# A tibble: 216 × 2\n    Year     n\n   &lt;dbl&gt; &lt;int&gt;\n 1  1790     2\n 2  1791     1\n 3  1792     1\n 4  1793     1\n 5  1794     1\n 6  1795     1\n 7  1796     1\n 8  1797     1\n 9  1798     1\n10  1799     1\n# ℹ 206 more rows\n\n\nLooking at the column President (and sorting it out by those with higher counts) indicates that\n\nspeeches |&gt; \n  count(President, sort = TRUE)\n\n# A tibble: 41 × 2\n   President                     n\n   &lt;chr&gt;                     &lt;int&gt;\n 1 Franklin Delano Roosevelt    12\n 2 Dwight D. Eisenhower          9\n 3 Andrew Jackson                8\n 4 Barack Obama                  8\n 5 Bill Clinton                  8\n 6 George W. Bush                8\n 7 George Washington             8\n 8 Grover Cleveland              8\n 9 Harry S. Truman               8\n10 James Monroe                  8\n# ℹ 31 more rows\n\n\nIt is also possible to have a look at the text of the speeches themselves. The code below for instance, takes the object data and prints the contents of fourth row of the field Text.\n\nspeeches$Text[4] \n\n[1] \"['Fellow-Citizens of the Senate and of the House of Representatives:', 'It is some abatement of the satisfaction with which I meet you on the present occasion that, in felicitating you on a continuance of the national prosperity generally, I am not able to add to it information that the Indian hostilities which have for some time past distressed our Northwestern frontier have terminated.', 'You will, I am persuaded, learn with no less concern than I communicate it that reiterated endeavors toward effecting a pacification have hitherto issued only in new and outrageous proofs of persevering hostility on the part of the tribes with whom we are in contest. An earnest desire to procure tranquillity to the frontier, to stop the further effusion of blood, to arrest the progress of expense, to forward the prevalent wish of the nation for peace has led to strenuous efforts through various channels to accomplish these desirable purposes; in making which efforts I consulted less my own anticipations of the event, or the scruples which some considerations were calculated to inspire, than the wish to find the object attainable, or if not attainable, to ascertain unequivocally that such is the case.', 'A detail of the measures which have been pursued and of their consequences, which will be laid before you, while it will confirm to you the want of success thus far, will, I trust, evince that means as proper and as efficacious as could have been devised have been employed. The issue of some of them, indeed, is still depending, but a favorable one, though not to be despaired of, is not promised by anything that has yet happened.', 'In the course of the attempts which have been made some valuable citizens have fallen victims to their zeal for the public service. A sanction commonly respected even among savages has been found in this instance insufficient to protect from massacre the emissaries of peace. It will, I presume, be duly considered whether the occasion does not call for an exercise of liberality toward the families of the deceased.', 'It must add to your concern to be informed that, besides the continuation of hostile appearances among the tribes north of the Ohio, some threatening symptoms have of late been revived among some of those south of it.', 'A part of the Cherokees, known by the name of Chickamaugas, inhabiting five villages on the Tennessee River, have long been in the practice of committing depredations on the neighboring settlements.', 'It was hoped that the treaty of Holston, made with the Cherokee Nation in July, 1791, would have prevented a repetition of such depredations; but the event has not answered this hope. The Chickamaugas, aided by some banditti of another tribe in their vicinity, have recently perpetrated wanton and unprovoked hostilities upon the citizens of the United States in that quarter. The information which has been received on this subject will be laid before you. Hitherto defensive precautions only have been strictly enjoined and observed.', 'It is not understood that any breach of treaty or aggression whatsoever on the part of the United States or their citizens is even alleged as a pretext for the spirit of hostility in this quarter.', 'I have reason to believe that every practicable exertion has been made (pursuant to the provision by law for that purpose) to be prepared for the alternative of a prosecution of the war in the event of a failure of pacific overtures. A large proportion of the troops authorized to be raised have been recruited, though the number is still incomplete, and pains have been taken to discipline and put them in condition for the particular kind of service to be performed. A delay of operations (besides being dictated by the measures which were pursuing toward a pacific termination of the war) has been in itself deemed preferable to immature efforts. A statement from the proper department with regard to the number of troops raised, and some other points which have been suggested, will afford more precise information as a guide to the legislative consultations, and among other things will enable Congress to judge whether some additional stimulus to the recruiting service may not be advisable.', 'In looking forward to the future expense of the operations which may be found inevitable I derive consolation from the information I receive that the product of the revenues for the present year is likely to supersede the necessity of additional burthens on the community for the service of the ensuing year. This, however, will be better ascertained in the course of the session, and it is proper to add that the information alluded to proceeds upon the supposition of no material extension of the spirit of hostility.', 'I can not dismiss the subject of Indian affairs without again recommending to your consideration the expediency of more adequate provision for giving energy to the laws throughout our interior frontier and for restraining the commission of outrages upon the Indians, without which all pacific plans must prove nugatory. To enable, by competent rewards, the employment of qualified and trusty persons to reside among them as agents would also contribute to the preservation of peace and good neighborhood. If in addition to these expedients an eligible plan could be devised for promoting civilization among the friendly tribes and for carrying on trade with them upon a scale equal to their wants and under regulations calculated to protect them from imposition and extortion, its influence in cementing their interest with ours could not but be considerable.', 'The prosperous state of our revenue has been intimated. This would be still more the case were it not for the impediments which in some places continue to embarrass the collection of the duties on spirits distilled within the United States. These impediments have lessened and are lessening in local extent, and, as applied to the community at large, the contentment with the law appears to be progressive.', 'But symptoms of increased opposition having lately manifested themselves in certain quarters, I judged a special interposition on my part proper and advisable, and under this impression have issued a proclamation warning against all unlawful combinations and proceedings having for their object or tending to obstruct the operation of the law in question, and announcing that all lawful ways and means would be strictly put in execution for bringing to justice the infractors thereof and securing obedience thereto.', 'Measures have also been taken for the prosecution of offenders, and Congress may be assured that nothing within constitutional and legal limits which may depend upon me shall be wanting to assert and maintain the just authority of the laws. In fulfilling this trust I shall count entirely upon the full cooperation of the other departments of the Government and upon the zealous support of all good citizens.', 'I can not forbear to bring again into the view of the Legislature the subject of a revision of the judiciary system. A representation from the judges of the Supreme Court, which will be laid before you, points out some of the inconveniences that are experienced. In the course of the execution of the laws considerations arise out of the structure of the system which in some cases tend to relax their efficacy. As connected with this subject, provisions to facilitate the taking of bail upon processes out of the courts of the United States and a supplementary definition of offenses against the Constitution and laws of the Union and of the punishment for such offenses will, it is presumed, be found worthy of particular attention.', 'Observations on the value of peace with other nations are unnecessary. It would be wise, however, by timely provisions to guard against those acts of our own citizens which might tend to disturb it, and to put ourselves in a condition to give that satisfaction to foreign nations which we may sometimes have occasion to require from them. I particularly recommend to your consideration the means of preventing those aggressions by our citizens on the territory of other nations, and other infractions of the law of nations, which, furnishing just subject of complaint, might endanger our peace with them; and, in general, the maintenance of a friendly intercourse with foreign powers will be presented to your attention by the expiration of the law for that purpose, which takes place, if not renewed, at the close of the present session.', 'In execution of the authority given by the Legislature measures have been taken for engaging some artists from abroad to aid in the establishment of our mint. Others have been employed at home. Provision has been made of the requisite buildings, and these are now putting into proper condition for the purposes of the establishment. There has also been a small beginning in the coinage of half dimes, the want of small coins in circulation calling the first attention to them.', 'The regulation of foreign coins in correspondency with the principles of our national coinage, as being essential to their due operation and to order in our money concerns, will, I doubt not, be resumed and completed.', 'It is represented that some provisions in the law which establishes the post office operate, in experiment, against the transmission of news papers to distant parts of the country. Should this, upon due inquiry, be found to be the fact, a full conviction of the importance of facilitating the circulation of political intelligence and information will, I doubt not, lead to the application of a remedy.', 'The adoption of a constitution for the State of Kentucky has been notified to me. The Legislature will share with me in the satisfaction which arises from an event interesting to the happiness of the part of the nation to which it relates and conducive to the general order.', 'It is proper likewise to inform you that since my last communication on the subject, and in further execution of the acts severally making provision for the public debt and for the reduction thereof, three new loans have been effected, each for 3,000,000 florins - one at Antwerp, at the annual interest of 4.5%, with an allowance of 4% in lieu of all charges, in the other 2 at Amsterdam, at the annual interest of 4%, with an allowance of 5.5% in one case and of 5% in the other in lieu of all charges. The rates of these loans and the circumstances under which they have been made are confirmations of the high state of our credit abroad.', 'Among the objects to which these funds have been directed to be applied, the payment of the debts due to certain foreign officers, according to the provision made during the last session, has been embraced.', '\\\\nGentlemen of the House of Representatives:', 'I entertain a strong hope that the state of the national finances is now sufficiently matured to enable you to enter upon a systematic and effectual arrangement for the regular redemption and discharge of the public debt, according to the right which has been reserved to the Government. No measure can be more desirable, whether viewed with an eye to its intrinsic importance or to the general sentiment and wish of the nation.', 'Provision is likewise requisite for the reimbursement of the loan which has been made of the Bank of the United States, pursuant to the eleventh section of the act by which it is incorporated. In fulfilling the public stipulations in this particular it is expected a valuable saving will be made.', 'Appropriations for the current service of the ensuing year and for such extraordinaries as may require provision will demand, and I doubt not will engage, your early attention.', '\\\\nGentlemen of the Senate and of the House of Representatives:', 'I content myself with recalling your attention generally to such objects, not particularized in my present, as have been suggested in my former communications to you.', 'Various temporary laws will expire during the present session. Among these, that which regulates trade and intercourse with the Indian tribes will merit particular notice.', 'The results of your common deliberations hitherto will, I trust, be productive of solid and durable advantages to our constituents, such as, by conciliating more and more their ultimate suffrage, will tend to strengthen and confirm their attachment to that Constitution of Government upon which, under Divine Providence, materially depend their union, their safety, and their happiness.', 'Still further to promote and secure these inestimable ends there is nothing which can have a more powerful tendency than the careful cultivation of harmony, combined with a due regard to stability, in the public councils.']\"\n\n\nBut, how do we extract information from this type of unstructured data?"
  },
  {
    "objectID": "speeches.html#word-counts",
    "href": "speeches.html#word-counts",
    "title": "Case-Study 2: The State of the Union Presidential Speeches",
    "section": "Word counts",
    "text": "Word counts\nOne possibility is to look at the number of times a particular term is mentioned in the corpus (or a set of terms). Let’s for instance count how many times de words “woman” and “women” show up in the presidential speeches. The code below uses mutate() to create two new variables woman and women indicating how many times those terms appear in the column Text. The actual computation is performed by the function str_count() (from the tidytext package). R goes through the text in that column and search for the string of characters indicated there. Although this is a very simplistic way of searching for terms, it serves as an illustration. Given that we are interested in both terms simultaneously, we can aggregate this information by constructing another column (sum_wom) summing both columns. We modify the existing object using the operator &lt;-.\n\nspeeches &lt;- speeches |&gt;\n  mutate(woman = str_count(Text, \"woman\"),\n         women = str_count(Text, \"women\"),\n         sum_wom = woman + women)\n\nTyping the name of the object shows the results. We have basically added columns indicating how many times those terms show up in each speech.\n\nspeeches\n\n# A tibble: 219 × 7\n   President          Year Title                       Text  woman women sum_wom\n   &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;                       &lt;chr&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt;\n 1 George Washington  1790 First State of the Union A… \"['I…     0     0       0\n 2 George Washington  1790 Second State of the Union … \"['F…     0     0       0\n 3 George Washington  1791 Third State of the Union A… \"['F…     0     0       0\n 4 George Washington  1792 Fourth State of the Union … \"['F…     0     0       0\n 5 George Washington  1793 Fifth State of the Union A… \"['F…     0     0       0\n 6 George Washington  1794 Sixth State of the Union A… \"['F…     0     0       0\n 7 George Washington  1795 Seventh State of the Union… \"['F…     0     1       1\n 8 George Washington  1796 Eighth State of the Union … \"['F…     0     0       0\n 9 by John Adams      1797 First State of the Union A… \"['I…     0     0       0\n10 by John Adams      1798 Second State of the Union … \"['G…     0     0       0\n# ℹ 209 more rows\n\n\nThis kind of information is a numerical variable that can be treated the same way as the ones we explored in the previous case-study. Imagine, for instance that you want to compute the total number of times those terms are mentioned in the whole corpus (all the speeches) plus the mean value (how many times they show up for speech, on average).\n\nspeeches |&gt;\n  summarize(sum = sum(sum_wom),\n            mean = mean(sum_wom, na.rm = TRUE))\n\n# A tibble: 1 × 2\n    sum  mean\n  &lt;int&gt; &lt;dbl&gt;\n1   350  1.60\n\n\nAlternatively, we can explore the evolution of the use of these words over time, that is, how often they show up by year. The peculiar structure of this corpus makes this a bit more complicated than it actually is: we have one speech by year, except in 1790 when we have two. This means that for that year, we have two values in the column sum_wom (one for each speech). If we want to show the information for that year, we need to make a decision: either we sum those values, average them or something. Alternatively, to make things simpler, we can just drop that year from the analysis using filter(). While ggplot() defines which columns are shown in the x- and y-axes (Year and sum_women, respectively), geom_line() indicates that we want to plot a line graph.\n\nspeeches %&gt;%\n  filter(Year&gt;1790) |&gt;\n  ggplot(aes(x = Year, y = sum_wom)) +\n  geom_line()\n\n\n\n\n\n\n\n\nThe previous analysis is a bit naive (among other issues). What if some speeches are longer than others? Having “women” mentioned more often in some of them may therefore not reflect the attention given to women but just the fact that those speeches are longer and therefore have more room for talking about more things. One way of dealing with this issue is to relativise the previous value depending on how long the speech is. The code below uses again mutate() and str_count() to create a column counting how many words each speech has. [\\\\w]+ is a regular expression (regex or regexp), that is, a sequence of characters that specifies a match pattern in text. Given that we don’t have time to explain this, just trust me on this (useful tools for regular expressions can be found here or here.\n\nspeeches &lt;- speeches |&gt;\n  mutate(word_count = str_count(Text, \"[\\\\w]+\"))\n\nWe can graph the results so you have a sense of what this exercise is doing. As shows below, speeches were very short during the first years and became longer over time.\n\nspeeches |&gt;\n  filter(Year&gt;1790) |&gt;\n  ggplot(aes(x = Year, y = word_count)) + \n  geom_point() + \n  geom_line()\n\n\n\n\n\n\n\n\nComing back to our original purpose, we are now in the position of relativising how many times the terms “woman” and “women” are mentioned depending on the length of the text. The code below does this operation (basically dividing the column sum_women between word_count) and plots the results.\n\nspeeches %&gt;% \n  mutate(sum_rel = sum_wom/word_count) |&gt;\n  filter(Year&gt;1790) %&gt;%\n  ggplot(aes(x = Year, y = sum_rel)) + \n  geom_point() + \n  geom_line()"
  },
  {
    "objectID": "speeches.html#top-frequencies",
    "href": "speeches.html#top-frequencies",
    "title": "Case-Study 2: The State of the Union Presidential Speeches",
    "section": "Top frequencies",
    "text": "Top frequencies\nInstead of searching for particular words (or set or words), we may want to adopt a more agnostic position and ask which words are most common in the speeches. This can be achieved by tokenizing the texts. This tool is actually used in many other applications, so it is important to see how it works.\nBasically, tokenizing splits the text into individual words (it also removes all of the punctuation and converts everything into lowercase characters). This is achieved with the function unnest_tokens() (which is part of the tidytext package). Apart from the object that contains the corpus we are exploring, this command requires two arguments: the column we want to tokenise (Text) and the name of the new column that will contain all the tokens (words in this case but it is up to you).\n\ndata_token &lt;- speeches |&gt; \n  unnest_tokens(output = words, input = Text)\n\nAs evident below, this tool transform the original corpus into a new dataframe when each row refers to each token, while preserving the metadata associated to them (speech, president, year, etc.). In total, we have almost 1.8 million tokens in this corpus.\n\ndata_token\n\n# A tibble: 1,766,436 × 8\n   President          Year Title            woman women sum_wom word_count words\n   &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;            &lt;int&gt; &lt;int&gt;   &lt;int&gt;      &lt;int&gt; &lt;chr&gt;\n 1 George Washington  1790 First State of …     0     0       0       1069 i    \n 2 George Washington  1790 First State of …     0     0       0       1069 embr…\n 3 George Washington  1790 First State of …     0     0       0       1069 with \n 4 George Washington  1790 First State of …     0     0       0       1069 great\n 5 George Washington  1790 First State of …     0     0       0       1069 sati…\n 6 George Washington  1790 First State of …     0     0       0       1069 the  \n 7 George Washington  1790 First State of …     0     0       0       1069 oppo…\n 8 George Washington  1790 First State of …     0     0       0       1069 which\n 9 George Washington  1790 First State of …     0     0       0       1069 now  \n10 George Washington  1790 First State of …     0     0       0       1069 pres…\n# ℹ 1,766,426 more rows\n\n\nOnce the data is expressed in this way, the column words contain all tokens mentioned in the speeches. These categories are basically qualitative information and can therefore be treated with the same tools we learned in the previous session. For instance, we can simply count() the number of times each category (token) is mentioned. Sorting the results by those with the highest frequencies (sort = TRUE) immediately indicates which tokens are the most common.\n\ndata_token |&gt; \n  count(words, sort = TRUE) %&gt;% \n  print(n = 20)\n\n# A tibble: 29,713 × 2\n   words      n\n   &lt;chr&gt;  &lt;int&gt;\n 1 the   149869\n 2 of     96455\n 3 to     60068\n 4 and    60026\n 5 in     38369\n 6 a      27709\n 7 that   21490\n 8 for    18917\n 9 be     18541\n10 is     16993\n11 our    16933\n12 it     14967\n13 by     14793\n14 which  12197\n15 as     12081\n16 this   12062\n17 with   11816\n18 have   11800\n19 we     11781\n20 i       9356\n# ℹ 29,693 more rows\n\n\nThe problem with this approach is that the most common words have little meaning (at least in terms of illustrating which topics are mentioned). Luckily, there is a simple solution, namely to get rid of those words that are so common that we are not interested in them, known as stop words. In fact there is a list of stop words already built within the tidy environment. Calling this object helps clarifying what stop words actually are: articles, prepositions, etc. We are not covering it but not only are there different lists of stop words (and in many different languages), but you can also modify them (add/remove terms) or create your own list from scratch).\n\nstop_words |&gt;   \n  print(n = 25)\n\n# A tibble: 1,149 × 2\n   word        lexicon\n   &lt;chr&gt;       &lt;chr&gt;  \n 1 a           SMART  \n 2 a's         SMART  \n 3 able        SMART  \n 4 about       SMART  \n 5 above       SMART  \n 6 according   SMART  \n 7 accordingly SMART  \n 8 across      SMART  \n 9 actually    SMART  \n10 after       SMART  \n11 afterwards  SMART  \n12 again       SMART  \n13 against     SMART  \n14 ain't       SMART  \n15 all         SMART  \n16 allow       SMART  \n17 allows      SMART  \n18 almost      SMART  \n19 alone       SMART  \n20 along       SMART  \n21 already     SMART  \n22 also        SMART  \n23 although    SMART  \n24 always      SMART  \n25 am          SMART  \n# ℹ 1,124 more rows\n\n\nThe trick now is to take this list of stop words and use it to exclude those terms from the tokenised version of the presidential speeches. In order to do so, we take the corpus and use anti_join() to drop the terms that match with the list of stop words. Note that the argument by defines which fields contain the terms to be matched (word in the object data_toke and words in the object stop_words).\n\ndata_token &lt;- data_token |&gt;\n  anti_join(stop_words, by = c(\"words\" = \"word\"))\ndata_token\n\n# A tibble: 696,002 × 8\n   President          Year Title            woman women sum_wom word_count words\n   &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;            &lt;int&gt; &lt;int&gt;   &lt;int&gt;      &lt;int&gt; &lt;chr&gt;\n 1 George Washington  1790 First State of …     0     0       0       1069 embr…\n 2 George Washington  1790 First State of …     0     0       0       1069 sati…\n 3 George Washington  1790 First State of …     0     0       0       1069 oppo…\n 4 George Washington  1790 First State of …     0     0       0       1069 cong…\n 5 George Washington  1790 First State of …     0     0       0       1069 favo…\n 6 George Washington  1790 First State of …     0     0       0       1069 pros…\n 7 George Washington  1790 First State of …     0     0       0       1069 publ…\n 8 George Washington  1790 First State of …     0     0       0       1069 affa…\n 9 George Washington  1790 First State of …     0     0       0       1069 rece…\n10 George Washington  1790 First State of …     0     0       0       1069 acce…\n# ℹ 695,992 more rows\n\n\nThe resulting object no longer contains those stop words. We can now proceed counting these categories again. The most common terms are now much more informative.\n\ndata_token |&gt; \n  count(words, sort = TRUE) |&gt;  \n  print(n = 20)\n\n# A tibble: 29,031 × 2\n   words          n\n   &lt;chr&gt;      &lt;int&gt;\n 1 government  6830\n 2 congress    5083\n 3 united      4784\n 4 people      3884\n 5 country     3374\n 6 public      3000\n 7 time        2820\n 8 war         2732\n 9 american    2640\n10 world       2307\n11 law         2124\n12 national    2122\n13 power       1938\n14 act         1860\n15 nation      1831\n16 peace       1796\n17 nations     1782\n18 citizens    1780\n19 service     1742\n20 system      1650\n# ℹ 29,011 more rows\n\n\nWe could easily graph this or even compare the most common words in different periods (or mentioned by different presidents, etc.). We are not going to enter in the details of the code below, but it basically depics which were the most common words before and after 1900.\n\ndata_token |&gt; \n  mutate(period = ifelse(Year &lt;= 1900, \"19th c.\", \"20th c.\")) |&gt; \n  group_by(period) |&gt; \n  count(words, sort=TRUE) |&gt; \n  mutate(proportion = n/sum(n)*1000) |&gt; \n  slice_max(order_by=proportion, n = 15) |&gt; \n  mutate(words = reorder(words, desc(proportion))) |&gt; \n  ggplot(aes(reorder_within(x = words, \n                            by = proportion, within = period),\n             proportion, fill = period)) + \n    geom_col() +\n    scale_x_reordered() +\n    scale_fill_manual(values = c(\"blue\", \"#56B4E9\")) +\n    coord_flip() +\n    facet_wrap(~period, ncol = 2, scales = \"free\") +\n    xlab(\"Word\")"
  },
  {
    "objectID": "speeches.html#n-grams",
    "href": "speeches.html#n-grams",
    "title": "Case-Study 2: The State of the Union Presidential Speeches",
    "section": "n-grams",
    "text": "n-grams\nAs mentioned above, tokenising goes beyond extracting what are the most commons words. It is also quite simple to count particular words as we did at the beginning of this session. The code below creates another column that assigns each observation the value 1 or 0 depending whether the token in the field words is “war” or not. It then computes the fraction of those tokens by year and plots the results.\n\ndata_token |&gt; \n  mutate(war = ifelse(words == \"war\", 1, 0)) |&gt;  \n  group_by(Year) |&gt;\n  summarize(fr_war = mean(war, na.rm = TRUE)) |&gt;\n  ggplot() +\n  geom_col(aes(x = Year, y = fr_war))\n\n\n\n\n\n\n\n\nLikewise, instead of splitting the corpus into 1-gram tokens, tokenising may involve creating multiple-word tokens: 2-grams, 3-grams, etc. As above, the code below uses unnest_tokens() again but it now indicates that we want 2-gram tokens (n = 2). The output shows how the corpus has not been split into.\n\ndata_token2 &lt;- speeches |&gt;\n  unnest_tokens(twogram, Text, token = \"ngrams\", n = 2)\ndata_token2\n\n# A tibble: 1,766,217 × 8\n   President          Year Title          woman women sum_wom word_count twogram\n   &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;          &lt;int&gt; &lt;int&gt;   &lt;int&gt;      &lt;int&gt; &lt;chr&gt;  \n 1 George Washington  1790 First State o…     0     0       0       1069 i embr…\n 2 George Washington  1790 First State o…     0     0       0       1069 embrac…\n 3 George Washington  1790 First State o…     0     0       0       1069 with g…\n 4 George Washington  1790 First State o…     0     0       0       1069 great …\n 5 George Washington  1790 First State o…     0     0       0       1069 satisf…\n 6 George Washington  1790 First State o…     0     0       0       1069 the op…\n 7 George Washington  1790 First State o…     0     0       0       1069 opport…\n 8 George Washington  1790 First State o…     0     0       0       1069 which …\n 9 George Washington  1790 First State o…     0     0       0       1069 now pr…\n10 George Washington  1790 First State o…     0     0       0       1069 presen…\n# ℹ 1,766,207 more rows\n\n\nThis type of multiple-word tokens are very useful to identify the terms that are mentioned accompanying particular words. Imagine, for instance, that we are not only interested in how many times the word “women” is mentioned, but also in which context. The code below implements such analysis.\n\nwomen &lt;- data_token2 |&gt; \n  separate_wider_delim(cols = twogram, delim = \" \", \n                       names = c(\"g1\", \"g2\")) |&gt;\n  filter(g1 == \"women\" | g2 == \"women\") |&gt;\n  pivot_longer(g1:g2) |&gt;\n  select(!name) |&gt;\n  rename(words = value) |&gt;\n  filter(words!=\"women\") |&gt;\n  anti_join(stop_words, by = c(\"words\" = \"word\")) \nwomen\n\n# A tibble: 84 × 8\n   President          Year Title            woman women sum_wom word_count words\n   &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;            &lt;int&gt; &lt;int&gt;   &lt;int&gt;      &lt;int&gt; &lt;chr&gt;\n 1 George Washington  1795 Seventh State o…     0     1       1       1988 inno…\n 2 Ulysses S. Grant   1873 Fifth State of …     0     1       1      10064 amer…\n 3 Ulysses S. Grant   1873 Fifth State of …     0     1       1      10064 marr…\n 4 Ulysses S. Grant   1874 Sixth State of …     0     2       2       9788 chin…\n 5 Ulysses S. Grant   1874 Sixth State of …     0     2       2       9788 amer…\n 6 Ulysses S. Grant   1875 Seventh State o…     0     3       3      12324 amer…\n 7 Ulysses S. Grant   1875 Seventh State o…     0     3       3      12324 chin…\n 8 Ulysses S. Grant   1876 Eighth State of…     0     1       1       6860 amer…\n 9 Grover Cleveland   1888 Fourth State of…     0     1       1      13200 170  \n10 Grover Cleveland   1888 Fourth State of…     0     1       1      13200 70   \n# ℹ 74 more rows\n\n\n\nwomen |&gt; \n  count(words, sort=TRUE)\n\n# A tibble: 65 × 2\n   words          n\n   &lt;chr&gt;      &lt;int&gt;\n 1 pregnant       7\n 2 american       6\n 3 serving        4\n 4 act            2\n 5 chinese        2\n 6 indian         2\n 7 infants        2\n 8 minorities     2\n 9 11             1\n10 170            1\n# ℹ 55 more rows\n\n\n\nwomen |&gt; \n  filter(words == \"pregnant\") |&gt;\n  count(Year)\n\n# A tibble: 3 × 2\n   Year     n\n  &lt;dbl&gt; &lt;int&gt;\n1  1981     4\n2  1989     1\n3  1995     2\n\n\nWe are going to stop here. We don’t have time for more but there is so much more. I hope this session has given you a sense of how computational methods allow extracting information from historical sources in a powerful way, regardless whether the information is qualitative, numerical or purely textual. For those who want to know more, see you in HIST2025 Computational History."
  },
  {
    "objectID": "backg-paisley.html",
    "href": "backg-paisley.html",
    "title": "Case-study 1: The Paisley House of Correction Dataset",
    "section": "",
    "text": "The fist case-study deals with the admission records from the Paisley prison, an institution located in a village near Glasgow (The Scottish National Archives). Kept in the National Archives of Scotland, these registers provide individual information on those prisoners who were admitted into this institution between 1841 and 1883.\nFor an illustration on how this type of source looks like, see the register included below of John Hearn, a 12-year old convicted in 1873 for stealing 11 pieces of leather who was sentenced to one month of hard labour. Interestingly, prison records provide info on both males and females, a feature that is relatively uncommon for this period (when most of the height data come from military conscripts).\nThe previous picture comes from the H.M. Prison Wandsworth, near London. The Paisley records are not as fancy. Figure 2 below shows a sample of the original source. While each column records different pieces of information (case number, date of admission, name, etc.), each row refers to each inmate in the data set. The data set was originally collected by Hamish Maxwell-Stewart, James Bradley and Tamsin O’Connor who systematically sample every fourth double page of all registers, resulting in a total of 13,879 observations. For practical reasons, we will rely on a subset of the full data set, containing a thousand prisoners.1 This source presents information in a very structured form, that can be easily transferred to a digital version.\nAs well as the case number, the source reports name, sex, age, place (and country) of birth, place of residence, height, weight, occupation and literacy level, etc. As well demographic and socio-economic information, the registers also include the offence committed and the sentence they were punished with (see image below). The education, occupations, and types of crimes (mostly petty crimes) suggest that these prisoners were drawn from the low working classes, which constituted a large proportion of the full population at the time. Comparing the prisoners’ registers with those of the 1861 population census also confirms that they were “ordinary if vulnerable workers” (Meredith and Oxley 2015, 209).\nInputing the raw data into an Excel spreadsheet results in Figure 3 below. Each column, known as field or variable presents a piece of information. As well as the case number (casen) and the date of admission (information that is split in two fields: month and year), the source records several pieces of information about these inmates, such name and surname, sex, age, place of birth (born) and country of birth (countryb), the place where the were living before being imprisoned (reside), height (in feet and inches) and weight, occupation (occup) and whether they were employed or not. It also reports their literacy, the marks that were visible in their bodies, the offence they committed and the sentence they received. While the first row displays the name of these variables, the remaining rows are devoted to each individual in the dataset.\nWho were these prisoners? Where they were coming from? Did prisoners’ occupations differ significantly from the rest of the population? What about literacy rates? Did men and women commit different crimes? Did judges treat everyone equally or did particular groups suffer harsher sentences? What explains the variation in stature and weights observed across prisoners? How did theses dimensions change during the period? The range of historical questions that this source can address is almost endless. Sarah Horrell, David Meredith and Deborah Oxley have relied on this information to significantly contribute to our understanding of 19th-century British society, especially regarding the biological living standards of the working classes and the gender dynamics that drove the allocation of resources within these families [Horrell and Oxley (2013); Meredith and Oxley (2015)).2 We strongly encourage reading those pieces to get to know more about the source and its possibilities. Bear in mind that, for practical reasons, we will rely on a subset of the full data set, containing a thousand prisoners.3\nThis type of source allows exploring many historical questions:\nSarah Horrell, David Meredith and Deborah Oxley have relied on this information to significantly contribute to our understanding of 19th-century British society, especially regarding the living standards of the working classes and gender dynamics that drove the allocation of resources within these families (Horrell and Oxley 2013; Meredith and Oxley 2015). In a seminal article, Sarah Horrell and Deborah Oxley had previously addressed these issues using similar registers from the Wandsworth prison, near London (Horrell, Meredith, and Oxley 2009). Those who are interested in knowing more about the source and its possibilities are encouraged to read those articles."
  },
  {
    "objectID": "backg-paisley.html#footnotes",
    "href": "backg-paisley.html#footnotes",
    "title": "Case-study 1: The Paisley House of Correction Dataset",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI am extremely thankful to Deborah Oxley and Hamish Maxwell-Stewart for kindly sharing this material. I am especially indebted to Deb whose course materials I inherited when I first started teaching a similar course at the University of Oxford in 2012.↩︎\nIn a seminal paper, Sarah Horrell and Deborah Oxley had previously addressed these issues using similar registers from the Wandsworth prison, near London (Horrell, Meredith, and Oxley 2009).↩︎\nWe are extremely grateful to Deborah Oxley and Hamish Maxwell-Stewart for kindly sharing the Paisley dataset.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantification in History - HIST1505/HIST1515",
    "section": "",
    "text": "March 10, 2025 – 10:15-12:00 – Auditorium D1\nFrancisco J. Beltrán Tapia\nWelcome to quantitative methods in history! As part of the course Introduction to historical theory and methods (HIST1505/HIST1515), this session provides a practical introduction on how historians can use computational tools to learn about the past.\nBelow are the instructions to prepare the session in advance:\n1. Install R and RStudio. R is a (free) statistical software and RStudio is an integrated interface that makes working with R easier. Bear in mind that you have to download and install both softwares: R and RStudio. The version you install depends on whether you are using Windows or Mac. Here is a link to access them.\n2. Create a dedicated folder to this session in your computer. Name it quants.\n3. Download the following materials. Store these files in separate folders (named data and scripts, respectively) within the folder quants:\n\nThe datasets we will be working with: here and here.\nThe R scripts containing the code to extract (some) information from them: here and here.\n\n4. Get familiar with R and RStudio by opening RStudio and following the instructions in the Intro to R section.\n5. Read the following texts in advance:\n\nThe background information for the two case-studies we will be exploring: The Paisley Prisoners’ Dataset and The State of the Union Presidential Speeches.\nThe text Quantifying history (available in Blackboard).\n\n6. Do not forget to bring your laptop to the session (or work in pairs)\nDo not hesitate to contact me if you have any question!"
  },
  {
    "objectID": "index.html#session-quantification-in-history",
    "href": "index.html#session-quantification-in-history",
    "title": "Quantification in History - HIST1505/HIST1515",
    "section": "",
    "text": "March 10, 2025 – 10:15-12:00 – Auditorium D1\nFrancisco J. Beltrán Tapia\nWelcome to quantitative methods in history! As part of the course Introduction to historical theory and methods (HIST1505/HIST1515), this session provides a practical introduction on how historians can use computational tools to learn about the past.\nBelow are the instructions to prepare the session in advance:\n1. Install R and RStudio. R is a (free) statistical software and RStudio is an integrated interface that makes working with R easier. Bear in mind that you have to download and install both softwares: R and RStudio. The version you install depends on whether you are using Windows or Mac. Here is a link to access them.\n2. Create a dedicated folder to this session in your computer. Name it quants.\n3. Download the following materials. Store these files in separate folders (named data and scripts, respectively) within the folder quants:\n\nThe datasets we will be working with: here and here.\nThe R scripts containing the code to extract (some) information from them: here and here.\n\n4. Get familiar with R and RStudio by opening RStudio and following the instructions in the Intro to R section.\n5. Read the following texts in advance:\n\nThe background information for the two case-studies we will be exploring: The Paisley Prisoners’ Dataset and The State of the Union Presidential Speeches.\nThe text Quantifying history (available in Blackboard).\n\n6. Do not forget to bring your laptop to the session (or work in pairs)\nDo not hesitate to contact me if you have any question!"
  },
  {
    "objectID": "readings.html",
    "href": "readings.html",
    "title": "Readings",
    "section": "",
    "text": "The article Quantifying history (available in Blackboard) is compulsory reading. It outlines the advantages and disadvantages of using quantitative and computational methods in history. We will be discussing this text at the beginning of the session before moving on and explore how quantitative methods are implemented in practice using two historical case-studies: the Paisley prison dataset and the State of the Union Addresses.\nIn case you are interested in knowing more about quantification in history, you can also check the articles available here.\n\nGraham et al. (2023), Historians Macroscope.\nFourie (2023), Quantitative History.\nLemercier and Zalc (2021), Back to the sources.\nBlaxill (2023), Why do historians ignore digital analysis.\nJockers (2012), Macroanalysis."
  },
  {
    "objectID": "get-r.html",
    "href": "get-r.html",
    "title": "Intro to R",
    "section": "",
    "text": "Implementing quantitative or computational analyses to historical (or any other) data requires some sort of statistical software. We will rely on R, a open-source free statistical software widely used by practitioners in many different fields both inside and outside academia. Although it is possible to work directly in R, using an integrated interface such as RStudio makes things much easier. RStudio basically integrates a text editor with the R console, so you can write, run and see the results of your analyses more easily.\nWe will be therefore download and install both software (R and RStudio) in your computer. Here is a link to access them (the version you install depends on whether you are using Windows or Mac)."
  },
  {
    "objectID": "get-r.html#r-interface",
    "href": "get-r.html#r-interface",
    "title": "Intro to R",
    "section": "R interface",
    "text": "R interface\nFigure 1 below illustrates what RStudio looks like. As well as the different menus at the top, the interface is divided in four panels. We will use the upper-left panel to write the code that instructs R to perform the analyses. You therefore need to open a new R Script (from the menu or clicking the icon). While it is also possible to directly type the commands in the console, using a script allows easily saving and replicating our work later on. While most of the results will show up in the console in the lower-left panel, the tab plots in the lower-right panel reproduces the visualisations we implement (by contrast, the tab files shows the structure of the directory where we are working on). Lastly, the environmment, located in the upper-left panel displays the objects that we load into the system.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: RStudio Interface.\n\n\n\nAs mentioned above, we will use a script to document our work. Using scripts makes it easy to keep a record of the commands you use, which in turn facilitates replicating (or adjusting) your analyse later, which will save you so much time (you can also re-use your old scripts or borrow parts to use in other projects)."
  },
  {
    "objectID": "get-r.html#setting-up-the-stage",
    "href": "get-r.html#setting-up-the-stage",
    "title": "Intro to R",
    "section": "Setting up the stage",
    "text": "Setting up the stage\nGiven that R needs some basic information regarding which folder in our computer (or the cloud) we are working from and which set of tools we are going to use, scripts usually start in a similar way. The code below includes some useful preparatory commands. You can copy and paste it in your own script.\n\n# Start afresh\nrm(list=ls()) # Clear de \"Global Environment\"\n\n# Set working directory\ngetwd() # Provides the current working directory\nsetwd(\"~/Documents/quants\") \n  # sets the working directory to the appropriate folder\n\n# Install packages\ninstall.packages(\"tidyverse\")\n\n# Load packages\nlibrary(tidyverse)\n\nNotice that the symbol # allows creating “comments” (the characters turn green). R does not “read” these lines when implementing the code, so they can be used to both better structure our scripts and comment the code itself.\nA couple of further clarifications are though in order. Firstly, it is often difficult to type the correct directory path (the one above is the one in my computer). Setting it manually through the menu helps properly selecting the folder you want to be working from: Session/Set working directory/Choose directory... Implementing this operation actually runs the necessary command in the console, so you can actually see the path to your folder or copy and paste it into the script (as it will be type in the script, you won’t need to do this again). I cannot emphasise enough how important this step is, since it indicates R where in our computer we will be working (so it is easy to access the necessary files). In this regard, it is important to have a folder structure (data, results, etc.) that facilitates navigating through your files.\nSecondly, the way that R works is by relying on tools that are contained in different packages. There are many of these packages and they not only need to be installed (only once), but also open before using (each session). This is why we need the commands install.packages() and library() to indicate which packages need to be installed and opened. The package tidyverse, in particular, gathers together different packages that are commonly used like ggplot2, dplyr, readr, etc. (it is therefore not necessary to install and load those packages individually). We will keep incorporating different packages as we need them.\nYour RStudio interface should look like something like Figure 2, except for the fact that the command setwd() should include the path to your own working directory. We can now run these lines of code selecting them and using the icon Run. Clicking it makes R go through those command lines in sequence (or through the whole script if you indicate so). It first clears the environment, then sets the working directory and lastly installs and opens the package tidyverse. The results of these commands happen in the console panel, which gives some comments that we can safely ignore for now.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Starting a R script."
  },
  {
    "objectID": "get-r.html#importing-data-sets",
    "href": "get-r.html#importing-data-sets",
    "title": "Intro to R",
    "section": "Importing data sets",
    "text": "Importing data sets\nAs well as the appropriate software (R and RStudio), this session uses two historical data sets to illustrate the concepts and methods covered here (you should have downloaded them). In order to start exploring this information, we first need to import the data into the R environment.\nAs an illustration, let’s focus on the Paisley dataset, one of the historical sources we will be exploring here. The raw data is stored as an excel spreadsheet named “paisley-data.xlsx” in the folder “data”. The command readxl() imports this excel file into the R environment. Although readxl is contained in the tidyverse package, you still need to open it explicitly using library().\n\nlibrary(readxl)\ndata &lt;- read_excel(\"data/paisley-data.xlsx\")\n\nNotice that we are instructing R to find the file that is located in a particular folder. We are using a relative path that stems from where your project is saved in (if the data is in the same working directory, you don’t need the path, just the file name).1 Notice also the symbol &lt;- (called assignment operator). It serves to create a (temporary) object, named data containing the Paisley data, which is now in the “environment” we are working with. The name of the object is up to you, we call it “data” but it could be anything else with certain restrictions (i.e. not starting with a number). One neat R feature is that you can load many objects simultaneously (with different pieces of information each) and treat them separately depending on your needs (we will see that we can “create” those objects ourselves as results of our analyses).\nOnce an object has been created containing the data frame, it is listed in the upper-right window called environment. Typing the name of the object (data in this case) provides a peak at the underlying information. As shown below, the upper left corner indicates that this data frame (referred to as a tibble in the tidyverse terminology) contains 1,000 individuals (rows) and 21 fields (columns). By default, typing the name of the object (data) only provides information on the 10 first cases in order to save memory and space (imagine that your dataset contains 10 million observations!). The number of fields that are displayed depends on how much space is available in the console in the lower part of the interface. You are nonetheless informed about the number of rows and variables that are not visible (as well as the names of those variables). Notice also that, below the variable name, R also indicates the type of variable: some are categorical (“chr” meaning character) and others are numerical (“dbl” meaning double).\n\ndata\n\n# A tibble: 1,000 × 21\n   casen    no month     year forename surname sex     age born  countryb reside\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt; \n 1     1    17 january   1841 AGNES    M'INTY… fema…    24 pais… scotland colin…\n 2     2    45 january   1841 CATHERI… CARLIN… fema…    30 irvi… scotland paisl…\n 3     3    68 january   1841 JEAN     WRIGHT  fema…    17 pais… scotland paisl…\n 4     4    91 february  1841 MARGRET  M'HAFF… fema…    18 glas… scotland paisl…\n 5     5    93 february  1841 JANET    M'LEAN  fema…    25 cath… scotland strab…\n 6     6   263 april     1841 ELIZA    DUNCAN  fema…    34 belf… ireland  paisl…\n 7     7   280 april     1841 ANN      RYLEY   fema…    45 sligo ireland  glasg…\n 8     8   299 april     1841 MARGRET  M'LEOD  fema…    40 gree… scotland green…\n 9     9   300 april     1841 MARY     MILLAR… fema…    19 gree… scotland green…\n10    10   310 april     1841 JEAN     M'KINL… fema…    29 glas… scotland pollo…\n# ℹ 990 more rows\n# ℹ 10 more variables: feet &lt;dbl&gt;, inches &lt;dbl&gt;, weight &lt;dbl&gt;, occup &lt;chr&gt;,\n#   employed &lt;chr&gt;, literacy &lt;chr&gt;, marks &lt;chr&gt;, offence &lt;chr&gt;, sentence &lt;chr&gt;,\n#   source &lt;chr&gt;\n\n\nIf you want to display more cases, you can use the function print(n = 15) and indicate the number of cases to be reported. Let’s pause for a moment to disect what the code below is doing. Notice first that it is crucial to indicate where the information you are asking for is coming from. Remember that we imported the Paisley data into the object name data. The pipe (|&gt;) here basically takes this object and uses it as input in the next line of code, which uses the function print() to request listing the first 15 cases from that object. Alternatively, you can have a look at the last 20 cases by typing tail(20).\n\ndata |&gt;  \n  print(n = 15)\n\n# A tibble: 1,000 × 21\n   casen    no month     year forename surname sex     age born  countryb reside\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt; \n 1     1    17 january   1841 AGNES    M'INTY… fema…    24 pais… scotland colin…\n 2     2    45 january   1841 CATHERI… CARLIN… fema…    30 irvi… scotland paisl…\n 3     3    68 january   1841 JEAN     WRIGHT  fema…    17 pais… scotland paisl…\n 4     4    91 february  1841 MARGRET  M'HAFF… fema…    18 glas… scotland paisl…\n 5     5    93 february  1841 JANET    M'LEAN  fema…    25 cath… scotland strab…\n 6     6   263 april     1841 ELIZA    DUNCAN  fema…    34 belf… ireland  paisl…\n 7     7   280 april     1841 ANN      RYLEY   fema…    45 sligo ireland  glasg…\n 8     8   299 april     1841 MARGRET  M'LEOD  fema…    40 gree… scotland green…\n 9     9   300 april     1841 MARY     MILLAR… fema…    19 gree… scotland green…\n10    10   310 april     1841 JEAN     M'KINL… fema…    29 glas… scotland pollo…\n11    11   343 may       1841 AGNES    CURRIE… fema…    35 islay scotland paisl…\n12    12   382 june      1841 ELLIZA   MUNN    fema…    18 john… scotland johns…\n13    13   425 june      1841 SARAH    BLACK … fema…    36 glas… scotland kelvi…\n14    57     3 january   1841 THOMAS   ROBERT… male     19 pais… scotland high …\n15    58    19 january   1841 JOHN     MONTGO… male     24 some… england  barra…\n# ℹ 985 more rows\n# ℹ 10 more variables: feet &lt;dbl&gt;, inches &lt;dbl&gt;, weight &lt;dbl&gt;, occup &lt;chr&gt;,\n#   employed &lt;chr&gt;, literacy &lt;chr&gt;, marks &lt;chr&gt;, offence &lt;chr&gt;, sentence &lt;chr&gt;,\n#   source &lt;chr&gt;\n\n\nThere are other commands that help knowing more about how the data set looks like such as glimpse() or names(). Feel free to try them out yourself. You can also have a sense of the magnitude and complexity of the whole data set by typing view(data). The latter opens up a new tab where you can explore the full data set at ease. Notice also that some values are missing. R signals them as NA (not available), meaning that no information is recorded in those fields for those observations. We will discuss the importance of missing values in due time. What it is important to stress now is that, although the Paisley data set is not especially big, scrolling up-down and left-right makes it obvious that it is extremely difficult to extract any kind of pattern by just “looking” at all this information. Here is where statistics (and R) come to the rescue. This is basically what this session will be about: a basic overview on how historians use computational methods to extract the rich information contained in our sources. If you have got this far, congrats, you are ready for it!"
  },
  {
    "objectID": "get-r.html#further-references",
    "href": "get-r.html#further-references",
    "title": "Intro to R",
    "section": "Further references",
    "text": "Further references\nAlexander, Rohan (2023), Telling stories with Data. With applications in R (CRC Press).\nIsmay, Chester and Kim, Albert Y. (2024), Statistical inference via Data Science. A ModernDive into R and the Tidyverse (CRC Press).\nSilge, Julia, and Robinson, David (2017), Text mining with R. A tidy approach (O’Reilly). Wickham, Hadley, Çetinkaya-Rundel, Mine, and Grolemund, Garret (2023), R for Data Science (O’Reilly; 2nd edition)."
  },
  {
    "objectID": "get-r.html#footnotes",
    "href": "get-r.html#footnotes",
    "title": "Intro to R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis will work assuming the working directory is set to the appropriate folder (see previous step above). Although we strongly advice not to use menus, finding the appropriate path to your files is not always straightforward, so we make an exception here.↩︎"
  },
  {
    "objectID": "paisley.html",
    "href": "paisley.html",
    "title": "Case-Study 1: The Paisley Prison dataset",
    "section": "",
    "text": "This case-study introduces basic tools to systematize and extract information from historical sources, regadless it is qualitative or numerical. Displaying frequency tables, plotting histograms and reporting summary statistics (i.e. the mean, the minimum and maximum values, the standard deviation, etc.) helps characterising how our data looks like. Making sense of the source using descriptive statistics can actually get you a long way in your understanding of the historical setting you are studying."
  },
  {
    "objectID": "paisley.html#setting-the-stage",
    "href": "paisley.html#setting-the-stage",
    "title": "Case-Study 1: The Paisley Prison dataset",
    "section": "Setting the stage",
    "text": "Setting the stage\nAs explained in the Intro to R section, we need to include some preliminary commands in our script so we (1) get rid of other objects that could be in the R environment from previous sessions, (2) set the working directory, (3) load (and install if necessary) the packages we pla to use, and (4) import the dataset.\n\n# Clear de \"Global Environment\"\nrm(list=ls()) \n\n# Sets the working directory\nsetwd(\"~/Documents/quants\") \n\n# Install/load packages\ninstall.packages(\"tidyverse\")\nlibrary(tidyverse)\nlibrary(readxl)\ndata &lt;- read_excel(\"data/paisley-data.xlsx\")"
  },
  {
    "objectID": "paisley.html#inspecting-the-data",
    "href": "paisley.html#inspecting-the-data",
    "title": "Case-Study 1: The Paisley Prison dataset",
    "section": "Inspecting the data",
    "text": "Inspecting the data\nOnce the data is imported into the R environment as an object, we can start inspecting it as shown in the Intro to R section. As shown in the Global Environment, in the upper left corner, this this data frame (referred to as a tibble in the tidyverse terminology) contains 1,000 individuals (rows) and 21 fields (columns). Typing the name of the object (data) only provides information on the 10 first cases in order to save memory and space. Notice also that, below the variable name, R also indicates the type of variable: some are categorical (“chr” meaning character) and others are numerical (“dbl” meaning double).\n\ndata\n\n# A tibble: 1,000 × 21\n   casen    no month     year forename surname sex     age born  countryb reside\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt; \n 1     1    17 january   1841 AGNES    M'INTY… fema…    24 pais… scotland colin…\n 2     2    45 january   1841 CATHERI… CARLIN… fema…    30 irvi… scotland paisl…\n 3     3    68 january   1841 JEAN     WRIGHT  fema…    17 pais… scotland paisl…\n 4     4    91 february  1841 MARGRET  M'HAFF… fema…    18 glas… scotland paisl…\n 5     5    93 february  1841 JANET    M'LEAN  fema…    25 cath… scotland strab…\n 6     6   263 april     1841 ELIZA    DUNCAN  fema…    34 belf… ireland  paisl…\n 7     7   280 april     1841 ANN      RYLEY   fema…    45 sligo ireland  glasg…\n 8     8   299 april     1841 MARGRET  M'LEOD  fema…    40 gree… scotland green…\n 9     9   300 april     1841 MARY     MILLAR… fema…    19 gree… scotland green…\n10    10   310 april     1841 JEAN     M'KINL… fema…    29 glas… scotland pollo…\n# ℹ 990 more rows\n# ℹ 10 more variables: feet &lt;dbl&gt;, inches &lt;dbl&gt;, weight &lt;dbl&gt;, occup &lt;chr&gt;,\n#   employed &lt;chr&gt;, literacy &lt;chr&gt;, marks &lt;chr&gt;, offence &lt;chr&gt;, sentence &lt;chr&gt;,\n#   source &lt;chr&gt;\n\n\nIf you want to display more cases, you can use the function print(n = 15) and indicate the number of cases to be reported. Let’s pause for a moment to disect what the code below is doing. Notice first that it is crucial to indicate where the information you are asking for is coming from. Remember that we imported the Paisley data into the object name data. The pipe (|&gt;) here basically takes this object and uses it as input in the next line of code, which uses the function print() to request listing the first 15 cases from that object. Alternatively, you can have a look at the last 20 cases by typing tail(20).\n\ndata |&gt;  \n  print(n = 15)\n\n# A tibble: 1,000 × 21\n   casen    no month     year forename surname sex     age born  countryb reside\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt; \n 1     1    17 january   1841 AGNES    M'INTY… fema…    24 pais… scotland colin…\n 2     2    45 january   1841 CATHERI… CARLIN… fema…    30 irvi… scotland paisl…\n 3     3    68 january   1841 JEAN     WRIGHT  fema…    17 pais… scotland paisl…\n 4     4    91 february  1841 MARGRET  M'HAFF… fema…    18 glas… scotland paisl…\n 5     5    93 february  1841 JANET    M'LEAN  fema…    25 cath… scotland strab…\n 6     6   263 april     1841 ELIZA    DUNCAN  fema…    34 belf… ireland  paisl…\n 7     7   280 april     1841 ANN      RYLEY   fema…    45 sligo ireland  glasg…\n 8     8   299 april     1841 MARGRET  M'LEOD  fema…    40 gree… scotland green…\n 9     9   300 april     1841 MARY     MILLAR… fema…    19 gree… scotland green…\n10    10   310 april     1841 JEAN     M'KINL… fema…    29 glas… scotland pollo…\n11    11   343 may       1841 AGNES    CURRIE… fema…    35 islay scotland paisl…\n12    12   382 june      1841 ELLIZA   MUNN    fema…    18 john… scotland johns…\n13    13   425 june      1841 SARAH    BLACK … fema…    36 glas… scotland kelvi…\n14    57     3 january   1841 THOMAS   ROBERT… male     19 pais… scotland high …\n15    58    19 january   1841 JOHN     MONTGO… male     24 some… england  barra…\n# ℹ 985 more rows\n# ℹ 10 more variables: feet &lt;dbl&gt;, inches &lt;dbl&gt;, weight &lt;dbl&gt;, occup &lt;chr&gt;,\n#   employed &lt;chr&gt;, literacy &lt;chr&gt;, marks &lt;chr&gt;, offence &lt;chr&gt;, sentence &lt;chr&gt;,\n#   source &lt;chr&gt;\n\n\nThere are other commands that help knowing more about how the data set looks like such as glimpse() or names(). Feel free to try them out yourself. You can also have a sense of the magnitude and complexity of the whole data set by typing view(data). The latter opens up a new tab where you can explore the full data set at ease. Notice also that some values are missing. R signals them as NA (not available), meaning that no information is recorded in those fields for those observations. We will discuss the importance of missing values in due time.\n\nview(data)\n\nWhat it is important to stress now is that, although the Paisley data set is not especially big, scrolling up-down and left-right makes it obvious that it is extremely difficult to extract any kind of pattern by just “looking” at all this information. Here is where statistics (and R) come to the rescue."
  },
  {
    "objectID": "paisley.html#categorical-qualitative-variables",
    "href": "paisley.html#categorical-qualitative-variables",
    "title": "Case-Study 1: The Paisley Prison dataset",
    "section": "Categorical (qualitative) variables",
    "text": "Categorical (qualitative) variables\nWe will start extracting information from the data set by focusing on categorical (qualitative) variables, those defined with words instead of numbers, such as sex, country of birth, occupation, etc. Each of these variables can exhibit certain values (categories) and it is important to stress that the difference between these values is merely qualitative (one category is no more or better than any other).\nThe first step is to assess how the distribution of values (categories) looks like, that is, to quantify their relative importance. A frequency table reports the number of observations (usually referred to as n) falling into each category, an information that can easily be retrieved using count() and indicating which variable you want to get information on.\n\ndata |&gt;  \n  count(sex)\n\n# A tibble: 2 × 2\n  sex        n\n  &lt;chr&gt;  &lt;int&gt;\n1 female   284\n2 male     716\n\n\nAs you will have guessed, the code above takes the object data that contains the Paisley data set and implements the function count() on the field sex. The results shows that the data contains 284 female prisoners and 716 male prisoners (the categories are presented in alphabetical order: “female” and “male”).\nWe could do the same with any other variable. Notice that the function count() also reports the number of missing values (if any). Reporting a frequency table of the variable employed yields three different categories: employed, unemployed and NA. In this case, the data set did not record the emplyment status of 821 prisoners. Analysing variables with missing values presents especial challenges because their accuracy depends on the reasons behind its “missingness”.\n\ndata |&gt;  \n  count(employed)\n\n# A tibble: 3 × 2\n  employed       n\n  &lt;chr&gt;      &lt;int&gt;\n1 employed     101\n2 unemployed    78\n3 &lt;NA&gt;         821\n\n\nAs with other commands listing information, count() only reports the first 10 categories by default. Some variables, such as occupation (occup) has many categories, so you probably want to inspect all of them. You can just indicate explicitly how many categories you want to display by using the function print(): while indicating n = 15 displays information on the first 15 categories, typing n = Inf reports all rows. Notice also that this command is now 3 lines long and we are using the pipe (|&gt;) in each line to implement this sequence of instructions: the pipe takes the output from that line and uses it as an input in the following line.\n\ndata |&gt;  \n  count(occup) |&gt;\n  print(n = 15)\n\n# A tibble: 209 × 2\n   occup             n\n   &lt;chr&gt;         &lt;int&gt;\n 1 at school         1\n 2 baker            15\n 3 barber            1\n 4 black smith      10\n 5 blacksmith        1\n 6 bleacher         13\n 7 block maker       1\n 8 block print       1\n 9 block printer     1\n10 boat builde       1\n11 boatman           7\n12 boatyard          1\n13 boiler make       3\n14 boiler maker      1\n15 boilermaker       2\n# ℹ 194 more rows\n\n\nAs the previous example using occupations show, frequency tables are not that useful when the variable contains a large number of categories. Also, by default, count() reports the different categories in alphabetical order which is often not particularly useful. In such case, it is better to present the categories according to their relative importance in order to quickly identify the most important categories. This is achieved by introducing the option sort and set it up as TRUE. The results indicate that the most common occupation was “labourer”, followed by “prostitute” and the rest. We also have 22 prisoners whose occupation was not recorded. You may wonder whether they did not want to report it or they did not have one. The same output can be achieved using the function arrange(desc(n)), which enables presenting the data in descending order based on the column n.\n\ndata |&gt;  \n  count(occup, sort = TRUE)\n\n# A tibble: 209 × 2\n   occup            n\n   &lt;chr&gt;        &lt;int&gt;\n 1 labourer       181\n 2 prostitute      81\n 3 weaver          40\n 4 carter          28\n 5 hawker          28\n 6 miner           28\n 7 house keeper    23\n 8 seaman          22\n 9 &lt;NA&gt;            22\n10 house keepe     16\n# ℹ 199 more rows\n\n\nThese simple descriptions of the data not only allow identifying what is typical, but also what is rare. In this regard, we can use arrange() to report those occupations that are less common among prisoners. Note that, by default, arrange() sorts the data in ascending order, so this command only needs the name of the field that serves to sort the data in ascending order (in the previous examples, we explicitly indicated that we wanted the data to be presented in descending order using the desc() argument). As seen below, it is also possible to extend the pipe (|&gt;) and use print() to report more categories if needed.\n\ndata |&gt;  \n  count(occup) |&gt; \n  arrange(n) |&gt; \n  print(n=15) \n\n# A tibble: 209 × 2\n   occup              n\n   &lt;chr&gt;          &lt;int&gt;\n 1 at school          1\n 2 barber             1\n 3 blacksmith         1\n 4 block maker        1\n 5 block print        1\n 6 block printer      1\n 7 boat builde        1\n 8 boatyard           1\n 9 boiler maker       1\n10 boot binder        1\n11 brick maker        1\n12 brothel keeper     1\n13 butcher            1\n14 cab driver         1\n15 cabinet making     1\n# ℹ 194 more rows\n\n\nLikewise, apart from the absolute number of observations falling in each category, it is useful to report the relative frequency. Frequency tables in fact routinely report both values. As explained above, count() effectively transforms the paisley data set into something different by summarising the info contained in a particular field. If we want to report the relative frequency, we need to add another column computing it. Therefore, we need to create a new variable using the command mutate() and instruct R how to populate that new field. As you will soon find out, mutate() is an extremely important command. In the example below, it takes the table created by count() as input and creates a new variable named prop (or something else; you decide which name you give to the new field). The value of the new field is something that the researcher sets up. In this case, we want to compute the relative frequency of each occupational category, that is, the number of cases falling into each category divided by the total number of observations. While the first value is the result of the function count() as reported in the field n, the second value can be retrieved by using the appropriate function. The operation n/sum(n) will therefore achieve what you need.\n\ndata |&gt;  \n  count(occup, sort = TRUE) |&gt;              \n  mutate(prop = n/sum(n)) \n\n# A tibble: 209 × 3\n   occup            n  prop\n   &lt;chr&gt;        &lt;int&gt; &lt;dbl&gt;\n 1 labourer       181 0.181\n 2 prostitute      81 0.081\n 3 weaver          40 0.04 \n 4 carter          28 0.028\n 5 hawker          28 0.028\n 6 miner           28 0.028\n 7 house keeper    23 0.023\n 8 seaman          22 0.022\n 9 &lt;NA&gt;            22 0.022\n10 house keepe     16 0.016\n# ℹ 199 more rows\n\n\nCrucially, you can narrow your analysis by focusing on particular subsamples of the data (or excluding outliers). This is achieved using filter() which allows “filtering” the data set according to the conditions that you specify. Imagine, for instance, that you are interested in knowing the educational background of the female prisoners. By specifying that the variable sex should be equal to “female”, filter() restricts the analysis to those observations (rows) fulfilling this condition. The results below show that, while many women in the Paisley data are either “illiterate” or “read a little”, only 7 “read & write well”.\n\ndata |&gt; \n  filter(sex==\"female\") |&gt;   \n  count(literacy, sort = TRUE) |&gt;              \n  mutate(prop = n/sum(n)) \n\n# A tibble: 10 × 3\n   literacy                   n    prop\n   &lt;chr&gt;                  &lt;int&gt;   &lt;dbl&gt;\n 1 read a little             94 0.331  \n 2 illiterate                79 0.278  \n 3 read & write a little     61 0.215  \n 4 &lt;NA&gt;                      18 0.0634 \n 5 cannot write              11 0.0387 \n 6 read & write well          7 0.0246 \n 7 read & write tolerably     5 0.0176 \n 8 read well                  4 0.0141 \n 9 read tolerably             3 0.0106 \n10 superior education         2 0.00704"
  },
  {
    "objectID": "paisley.html#numerical-variables",
    "href": "paisley.html#numerical-variables",
    "title": "Case-Study 1: The Paisley Prison dataset",
    "section": "Numerical variables",
    "text": "Numerical variables\nLet’s now explore variables that are expressed using numerical values. In the Paisley dataset, we only have three numerical variables: age, height and weight. Their specific properties advice to analyse them using a wider set of statistics. In this regard, simply reporting frequencies is often not very useful, especially when these variables include a large range of values. See, for instance, what happens when construct frequency table using now the variable age. The results report the number of observations falling in each category: 2 prisoners aged 9 years, 3 aged 10, etc. (we have also used mutate() to create an additional column with the relative frequency expressed as percentages). For questions of space, we just report the first 10 rows but the point is clear. You can display the full table, containing 62 rows, by adding the option print(n=Inf). The full table obviously provides interesting insights but it is simply too large for being useful as an interpretative tool.\n\ndata |&gt;  \n  count(age) |&gt;              \n  mutate(perc = 100*n/sum(n))\n\n# A tibble: 62 × 3\n     age     n  perc\n   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n 1     9     2   0.2\n 2    10     3   0.3\n 3    11     4   0.4\n 4    12     9   0.9\n 5    13     6   0.6\n 6    14    18   1.8\n 7    15    17   1.7\n 8    16    18   1.8\n 9    17    30   3  \n10    18    48   4.8\n# ℹ 52 more rows\n\n\nA useful way of exploring and reporting numerical variables is by using a histogram. This type of plot provides a visual representation of the distribution of values of the variable that we are analysing. The command ggplot() easily allows constructing histograms. You first need to indicate which variable you want to depict in the x-axis and then decide which type of plot you want. We have also included a line of code that establishes how the x-axis is labelled (in this case in multiples of 10 starting at age 0 and ending at age 90). As with any other function, we can also first use filter() to narrow down the analysis to particular subsamples of our data (i.e. gender, country of birth, etc.).\n\ndata |&gt;   \n  ggplot(aes(x = age)) +\n    geom_histogram(binwidth = 5) +\n    scale_x_continuous(breaks = seq(0, 90, 10))\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nHistograms present visual representations of the distribution of numerical variables. They are extremely useful because they provide an all-encompassing view of the data under analysis. However, it is also important to report specific values that help accurately describing the distribution. Descriptive statistics reduce complex distributions to more simple and intelligible numbers, thus making comparing distributions easier. The mean (or the average) is the most popular descriptive statistic but, depending on the researcher’s aim, other statistics may prove even more important.\nThe command summarise() allows computing all these descriptive statistics, also known as summary statistics. The following, for instance, compute the prisoners’ average age. This computation will generate a variable (we have assigned it here the name mean_age but you can choose any other name) that is equal to the function we specify. In this case, we want to compute the average, so we use the function mean(). Notice that we are also including the parameter na.rm = TRUE, which stands for “NA remove”, in order to exclude missing values from the computations and ensure accurate results (otherwise it results in NA because it cannot be computed; you can check the results removing that condition). As shown below, our prisoners are relatively young, on average.\n\ndata |&gt; \n  summarize(mean_age = mean(age, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  mean_age\n     &lt;dbl&gt;\n1     29.6\n\n\nCalling different functions within summarize() allows calculating other statistics. In the example below, we also compute the number of observations with information of age and the minimun and maximum values. We are using the functions sum(), mean(), min() and max() to compute the corresponding statistics.1 This exercise tells us that the average age (mean) of the 999 prisoners reporting age (obs) is 29.6 years. It also indicates that there is at least a prisoner as young as 9 years old (min) and at least another one as old as 89 (max).\n\ndata |&gt; \n  summarize(\n    obs = sum(!is.na(age)),\n    mean = mean(age, na.rm = TRUE), \n    min = min(age, na.rm = TRUE),\n    max = max(age, na.rm = TRUE)) \n\n# A tibble: 1 × 4\n    obs  mean   min   max\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   999  29.6     9    89"
  },
  {
    "objectID": "paisley.html#bivariate-statistics",
    "href": "paisley.html#bivariate-statistics",
    "title": "Case-Study 1: The Paisley Prison dataset",
    "section": "Bivariate statistics",
    "text": "Bivariate statistics\nHistorians and other social scientists routinely base their narratives on comparisons across different dimensions (gender, age, socio-economic groups, regions, etc.). Let’s then try to describe two variables simultaneously.\nImagine, for instance, that we want to provide information on age and adult weight. Notice that weight is expressed in pounds. For those of us more used to deal with the metrical system, it is advisable to change the unit of measurement. This can be done by creating another variable (weight_kg) that makes the conversion (1 kgs. = 0.453592 pounds; the operator &lt;- instruct R to modify the object data accordingly).\n\ndata &lt;- data |&gt;\n  mutate(weight_kg = 0.453592*weight)\n\nComing back to our main purpose of simultaneously looking at age and weight, it would not make sense to report a table listing the average height for each age (i.e. 20, 21, 22, etc.), so we first use mutate() to create a variable grouping age into different class intervals (in 10-year cohorts starting at age 9) and then compute the average for each group using group_by() and summarise(). Given that we want to focus on adult weight, we are restricting the analysis to those age 20 or older. The results clearly show that older prisoners tend to have lower weights on average.2\n\ndata |&gt; \n  filter(age&gt;=20) |&gt;\n  mutate(age_class = cut(age, breaks = seq(19, 89, 10))) |&gt;  \n  group_by(age_class) |&gt;\n  summarise(obs = sum(!is.na(weight_kg)),\n            mean_weight = mean(weight_kg, na.rm = TRUE))\n\n# A tibble: 7 × 3\n  age_class   obs mean_weight\n  &lt;fct&gt;     &lt;int&gt;       &lt;dbl&gt;\n1 (19,29]     331        63.7\n2 (29,39]     167        62.4\n3 (39,49]     104        62.6\n4 (49,59]      49        59.7\n5 (59,69]      21        57.2\n6 (69,79]       4        55.1\n7 (79,89]       1        44.5\n\n\nThe same information could be depicted using a line graph by using geom_line() in ggplot() and indicating to plot the variables we just computed (age and mean_weight) in the x- and y-axes, respectively. Notice that instead of grouping ages into 10-year intervals, we are using the full distribution of ages (the trade-off is the higher variation arising from the low number of observations for each individual age). Contrary to long tables containing all the categories, plots allow presenting all the information in a more concise way. The graph below tracks the relationship between these two dimensions and clearly suggests that getting older is associated with losing weight (or, at least, that the older prisoners in our data set are lighter than the younger ones).\n\ndata |&gt; \n  filter(age&gt;=20) |&gt;\n  group_by(age) |&gt;\n  summarise(mean_weight = mean(weight_kg, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = age, y = mean_weight, group = 1)) +\n    geom_line()\n\n\n\n\n\n\n\n\nWe could obviously refine our analysis to take into account other dimensions of our data. Using filter(), for instance, would allow to focus on particular subsamples of our data (i.e. males or females) or exclude outliers, that is, observations with extreme values that may distort our results (i.e. very old prisoners; see chapter X for a more detailed discussion of outliers). We can also go beyond the previous graph and simultaneously consider other dimensions in the visualisation itself. The code below replicates the previous plot but distinguishing by sex and excluding the prisoner who are really old (80+). This exercise not only makes clear that women are in general lighter, but also that they seem to lose weight more rapidly than men: the slope of the line tracking the relationship between age and weight is steeper.\n\ndata |&gt;\n  filter(age&gt;=20 & age&lt;80) |&gt;\n  group_by(age, sex) |&gt;\n  summarise(mean_weight = mean(weight_kg, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = age, y = mean_weight, color = sex)) +\n    geom_point() +\n    geom_line()\n\n\n\n\n\n\n\n\nWe don’t have time for more. This is though just the very tip of the iceberg. Quantitative tools allow extracting information from historical sources in a powerful way, regardless whether the information is numerical or qualitative."
  },
  {
    "objectID": "paisley.html#footnotes",
    "href": "paisley.html#footnotes",
    "title": "Case-Study 1: The Paisley Prison dataset",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt is important to notice the comma after each function since it allows for computing additional statistics (the last one thus does not need the comma). Note also that the brackets need to be balanced. Likewise, the function requesting the number of observations (sum()) is structured differently than the rest. This is because we need to count the number of prisoners reporting age (the option !is.na effectively ask to only consider those observations who are not reporting a missing value in the variable age). If we had used the code sum(age), na.rm = TRUE as in the other command lines, we would have obtained the sum of all the prisoners’ ages (29,562 years; you can test it yourself).↩︎\nThere are only a few very really old prisoners, so their average weight is also very much influenced by chance.↩︎"
  },
  {
    "objectID": "backg-speeches.html",
    "href": "backg-speeches.html",
    "title": "Case-study 2: The State of the Union Presidential Speeches",
    "section": "",
    "text": "The second case-study involves exploring the State of the Union Addresses that the president of the United States delivers annually since 1790. Each of these speeches seeks to set the political agenda.\nAs an illustration, please read the text included in the link below that records the address that Woodrow Wilson gave in December of 1913:\nState of the Union Presidential Address - Woodrow Wilson - December 2, 1913\nThe full corpus contains 235 texts (speeches; with a total of almost 1.7 million words) and constitutes an important source of information about the US political agenda and the wider socio-economic and cultural context surrounding them. This information has been gathered together into a .csv file that looks like Figure 1 below. Instead of columns, comma-separate (.csv) files separate the different pieces of information using commas as delimiters: name of the president delivering the speech (President), the year the speech was delivered (Year), the title of the speech (Title) and the (whole) text of the speech itself (Text).1 The first row displays the name of these variables and the remaining rows are devoted to each observation (speech) in the dataset.\nComputational text analysis can provide important insights about the contents of these speeches, including how they have changed over time or how they differ between Democrat and Republican presidents. What kind of words are used more often: “war” or “peace”, “justice” or “freedom”? Is education an important topic in these speeches? What about the economy (“business”, “debt”, “dollar”)? Which locations (including countries) are mentioned more often? Are women present in these speeches? What is the context in which these terms appear?\nIn this case-study, we will explore very simple tools that allow counting the number of particular words appearing in those speeches. For those eager to learning more after our session, you can read more about applying computational methods to this corpus here, here, here, here or here."
  },
  {
    "objectID": "backg-speeches.html#footnotes",
    "href": "backg-speeches.html#footnotes",
    "title": "Case-study 2: The State of the Union Presidential Speeches",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAn extended dataset including party affiliation is available here: https://www.kaggle.com/datasets/rtatman/state-of-the-union-corpus-1989-2017↩︎"
  }
]